# 为什么从0开始？

>确保在开始这节之前，你已经在这个星期完成了CS50 week0的内容，包括讲座，笔记，作业。

## 计算思维
在讲座中，教授首先给我们引入的是如下这张图：

![](/images/Pasted image 20251117114546.png)

计算机的本质。
计算机编程接受输出，产生输出，从而解决问题。
而输出和输入之间的过程就是，也就是我们常说的“黑盒”


**"但是，计算机如何'理解'输入？如何'表达'输出？**

当你在键盘上按下字母"A"，计算机会显示什么？
当你上传一张自拍照片，计算机储存的是什么？
当你播放一首歌，计算机处理的是什么？

答案:  **ALL NUMBER**

计算机面对的输入千变万化：**但黑箱内部必须有一种统一的'语言'来处理所有这些！**

### 为什么二进制？

假设你要设计一个能存储数字的物理装置：

|方案|实现方式|难度|
|---|---|---|
|**十进制**|需要10种不同的电压 (0V, 1V, 2V...9V)|⚠️ 很难准确区分|
|**二进制**|只需2种状态：高电压/低电压，开/关|✅ 超级简单！|
**从现实意义讲**:
计算机的基本原件（晶体管）天然就是开关，完美适配二进制，如果用十进制，需要精确区分10个电压级别，误判率会极高
**从数学意义讲**:
- 布尔代数（Boolean Algebra）完美适配
- 所有逻辑运算都可以用AND、OR、NOT实现
- 1854年乔治·布尔就为二进制铺好了理论基础

二进制不是计算机科学家的偏好，而是物理设备的选择

1. **黑箱需要处理各种输入** ↓
2. **必须有统一的表示方法** ↓
3. **数字是最通用的语言** ↓
4. **二进制是物理上最可靠的数字系统** ↓
5. **有了二进制，黑箱才能统一处理一切！**


### **历史趣事**

- 1930年代，苏联曾研制**三进制计算机** Setun，理论上效率更高
- 但由于制造困难、成本高昂，最终被二进制淘汰
- **量子计算机**是唯一的例外，使用量子比特（qubit），可以同时是0和1

---

# 二进制

讲座中戴维教授的灯泡和招募志愿同学已经讲解的很生动了，每年我们都可以在讲座中看到（笑）

### **1字节 = 8位二进制 = 256种可能**

![](/images/Pasted image 20251117172237.png)
这里衍生的概念如果大家是第一次接触可以和大家详细讲一下：

1. bit（位）--信息的原子
2. 全程：**Binary Digit**（二进制数字）
3. 只能是0**或**1
本质上Bit回答的是一个“是/否”问题

类比：像物理世界的"原子" — 不能再分割的最小单位

```
灯是开着的吗？ → 1 bit 信息（开=1，关=0）
今天下雨吗？   → 1 bit 信息（下=1，不下=0）
这个开关打开了吗？→ 1 bit 信息
```


关于中文翻译：**位 = 比特 = Bit**
**工程师常说**：
- "这个数据占 8 位" = "这个数据占 8 比特" = "这个数据是 8 bit"
- 三种说法完全等价

**每增加 1 bit，能表示的可能性翻倍**

# 用途：

## 衡量信息量

|Bits|可表示的状态数|实际例子|
|---|---|---|
|1 bit|2¹ = 2 种|开/关，是/否|
|2 bits|2² = 4 种|东南西北，春夏秋冬|
|3 bits|2³ = 8 种|一周七天（还多一个）|
|8 bits|2⁸ = 256 种|一个字符（A-Z, 0-9, 符号...）|
|24 bits|2²⁴ = 16,777,216 种|一个像素的颜色（RGB）|


**思考题**：
- 用 1 bit 能表示 0-3 吗？
- 用 2 bits 能表示 0-3 吗？
- 用 10 bits 能表示多少个不同的数？

## 衡量数据大小

```
8 bits = 1 Byte（字节）
1024 Bytes = 1 KB
1024 KB = 1 MB
1024 MB = 1 GB
```

**实际例子**：

|数据|大小|说明|
|---|---|---|
|字母 'A'|8 bits = 1 Byte|ASCII 编码|
|汉字 '你'|24 bits = 3 Bytes|UTF-8 编码|
|小头像图片|100 KB ≈ 819,200 bits|约 80 万个 0 和 1！|
|3分钟音乐|3 MB ≈ 25,165,824 bits|约 2500 万个 0 和 1！|

**震撼事实**：
> 你发一条"你好"的微信（2个汉字 = 6 Bytes = 48 bits），就是在传输 48 个精确排列的 0 和 1！

## 理解计算机性能
**处理器位数**：
- **32位处理器**：一次能处理 32 bits 数据
- **64位处理器**：一次能处理 64 bits 数据
- → 64位处理器理论上快一倍

## 信息论基础

额外引申：
**香农的信息熵**：

> "抛硬币的结果" = 1 bit 信息  
> "掷骰子的结果" = log₂(6) ≈ 2.58 bits 信息

**实际应用**：

- **压缩算法**：把冗余信息去掉，减少 bit 数量
    
    - 原文件：1,000,000 bits
    - 压缩后：300,000 bits
    - 压缩率：70%
- **加密**：增加破解所需的 bit 猜测量
    
    - 128位密钥 = 2¹²⁸ ≈ 3.4×10³⁸ 种可能
    - 暴力破解需要数十亿年

---
## byte
这个很容易和bit混淆

我们再次强调一下：
### **定义**：
- **1 Byte = 8 bits**
- 计算机存储和处理数据的**基本单位**
- 可以表示 2⁸ = **256** 种不同的状态



如果 Bit 是"原子"，那么 Byte 就是"分子" — 8个原子组成的最小有意义的单位

## 为什么是 8 bits？为什么不是 10 或 6？

### **历史原因 + 实用性**

#### **1. 表示字符的需要** 

**ASCII 码表的诞生**（1963年）：

- 英文字母：A-Z（26个）+ a-z（26个）= 52个
- 数字：0-9（10个）
- 标点符号：! @ # $ % 等（约30个）
- 控制字符：回车、换行、退格等（约30个）

**总共需要**：约 128 个字符

**计算**：

- 7 bits → 2⁷ = 128 ✅ 刚好够！
- 6 bits → 2⁶ = 64 ❌ 不够
- 8 bits → 2⁸ = 256 ✅ 有余量，更好！


**最终选择**：8 bits = 1 Byte
- 7 bits 用来表示字符
- 1 bit 留作**校验位**（检查传输错误）

#### **2. 硬件设计的便利** 

**2 的幂次方的优势**：

```
8 = 2³ → 电路设计简单

10 → 不是2的幂次方，电路复杂

地址总线：

8 bits → 256 个地址

16 bits → 65,536 个地址（= 256 × 256）

32 bits → 4,294,967,296 个地址（= 256⁴）

电路实现：

 8条线路并行传输
 每条线路传 1 bit
```


思考题：  **为什么汉字需要 3 Bytes，英文只要 1 Byte？**
		**Byte 能改成 16 bits 吗？**


关于8进制和16进制的内容大家可以转跳到哈吉凉的页面（）


## 编码
这里给大家列一些关键的时间线和拓展阅读，有兴趣可以自行查看
#### **ASCII 时代（1960s）**
[ASCII Table - ASCII Character Codes, HTML, Octal, Hex, Decimal](https://www.asciitable.com/)

最令人惊艳的是设计哲学

```
ASCII 字符布局（十进制）：

控制字符： 0-31 (00000000 - 00011111)

空格/符号： 32-47 (00100000 - 00101111)

数字： 48-57 (00110000 - 00111001) ← 注意这里！

符号： 58-64 (00111010 - 01000000)

大写字母： 65-90 (01000001 - 01011010) ← A 从 65 开始

符号： 91-96

小写字母： 97-122 (01100001 - 01111010) ← a 从 97 开始

符号： 123-127

```
由此可见，ASCII只用了1个字节（8bit）中的后7位，可以表示128个字符（0-127）
最让我们眼前一亮的是："a 是 1"，但是我们仔细看可以发现更多优雅的地方： **二进制视角：48 的魔法**


```
字符 '0' = 48 = 0b00110000
字符 '1' = 49 = 0b00110001
字符 '2' = 50 = 0b00110010
字符 '3' = 51 = 0b00110011
字符 '4' = 52 = 0b00110100
字符 '5' = 53 = 0b00110101
字符 '6' = 54 = 0b00110110
字符 '7' = 55 = 0b00110111
字符 '8' = 56 = 0b00111000
字符 '9' = 57 = 0b00111001
```
发现规律了吗？
#### **低 4 位就是数字本身！**

```
'0' = 00110000 → 低4位 = 0000 = 0

'1' = 00110001 → 低4位 = 0001 = 1

'2' = 00110010 → 低4位 = 0010 = 2

'3' = 00110011 → 低4位 = 0011 = 3

'4' = 00110100 → 低4位 = 0100 = 4

'5' = 00110101 → 低4位 = 0101 = 5

'6' = 00110110 → 低4位 = 0110 = 6

'7' = 00110111 → 低4位 = 0111 = 7

'8' = 00111000 → 低4位 = 1000 = 8

'9' = 00111001 → 低4位 = 1001 = 9

高4位永远是 0011 (=48的高位)

```

因此你可以将任何字符与字符零的值进行**异或**操作，以判断它是否是十进制数字（0 到 9）

```c
char c = '4';

// 判断是否是数字字符

bool is_digit = ((c ^ '0') & 0xF0) == 0;

// 原理：

// 数字字符：高4位永远是 0011

// XOR '0' 后：高4位变成 0000

// 与 0xF0：检查高4位是否全0

// 如果是 → 数字字符
```

或者更为高效的版本:
```c
// 判断并转换
char c = '8';
int digit = -1;

if ((c & 0xF0) == 0x30) {  // 高4位是否是 0011
    digit = c & 0x0F;       // 取低4位
}

// 一次判断 + 一次与运算

```


我们再把目光放在大小写字母
```
大写字母：
'A' = 65  = 0b01000001
'B' = 66  = 0b01000010
'C' = 67  = 0b01000011
...
'Z' = 90  = 0b01011010

小写字母：
'a' = 97  = 0b01100001
'b' = 98  = 0b01100010
'c' = 99  = 0b01100011
...
'z' = 122 = 0b01111010

差值：
'a' - 'A' = 97 - 65 = 32 = 0b00100000

```
#### **惊人发现：只差一个 bit**

```
'A' = 0b01000001

'a' = 0b01100001
          ↑

只有这一位不同！(第6位，从右数第6个)

所有对应的大小写字母，都只差这一位！
```

所以这里可以埋下一个伏笔，如果之后我们遇到需要大小写转换的时候，我们传统的方法可能是：

```c
char upper = 'A';

char lower = upper + 32; // 'A' + 32 = 'a'

char lower2 = 'b';

char upper2 = lower2 - 32; // 'b' - 32 = 'B'
```

而我们直接使用位操作：
```c
// 转小写：设置第6位为1
char to_lower(char c) {
    return c | 0x20;  // 0x20 = 0b00100000
}

// 转大写：清除第6位为0
char to_upper(char c) {
    return c & ~0x20; // ~0x20 = 0b11011111
}

// 切换大小写：翻转第6位
char toggle_case(char c) {
    return c ^ 0x20;  // XOR 翻转
}

// 示例：
'A' | 0x20  = 0b01000001 | 0b00100000 = 0b01100001 = 'a' ✅
'a' & ~0x20 = 0b01100001 & 0b11011111 = 0b01000001 = 'A' ✅
'A' ^ 0x20  = 0b01000001 ^ 0b00100000 = 0b01100001 = 'a' ✅

```

1960年代，计算机资源极其宝贵：

 内存按 KB 计算

 CPU 按 MHz 计算

 每个操作都要考虑效率

位操作 vs 算术运算：

- 位操作：1 个时钟周期

- 加减法：可能需要多个时钟周期（考虑进位）

所以：能用位操作，就不用算术

但是虽然ASCII虽然很好，但是也有我们停下来考虑的问题：
这对美国人来说足够了。但是，当计算机传到欧洲、亚洲时，问题来了：

- 法语里的 é 怎么办？
- 俄语字母怎么办？
- **我们中文的几万个汉字怎么办？**
1 个字节最多只能表示 256 种状态($2^8$)，根本装不下全世界的语言。

**解决方案：Unicode（万国码）**  
于是，科学家们搞了一个巨大的“字典”，叫 **Unicode**。它给世界上的每一个字符（无论是中文、日文、Emoji）都分配了一个唯一的编号。

- A 的编号还是 65（为了兼容）。
    
- 汉字中的编号是 20013（十六进制 4E2D）。
    
- Emoji 😂 的编号是 128514。

关键问题来了：编号有了，怎么存进计算机里？

如果所有字符都强行用 4 个字节存（为了容纳那个最大的编号），那存一个字母 A 也要 4 个字节，前面全是 0，**太浪费空间了！**（文件体积会膨胀 4 倍）
**UTF-8 就是为了解决“如何既能存下所有的字符，又不浪费空间”这个问题而诞生的。**

---
### UTF-8 的核心设计思路：可变长编码

UTF-8 的全称是 **8-bit Unicode Transformation Format**。它的核心思想是：**看人下菜碟**。

- 如果你是 **英文**（ASCII 字符），我就只用 **1 个字节** 存你。
    
- 如果你是 **带有音标的欧洲字符**，我就用 **2 个字节**。
    
- 如果你是 **常用汉字**，我就用 **3 个字节**。
    
- 如果你是 **生僻字或 Emoji**，我就用 **4 个字节**。
    

这就是所谓的**可变长度”**。

###  UTF-8 的原理：怎么知道一个字占几个字节？

计算机只认识 0 和 1。当它读取一串长长的二进制流时，它怎么知道哪里是一个字的开始，哪里是结束？

比如：0100000111100100...  
计算机怎么知道是读 8 位停下来，还是读 16 位停下来？

UTF-8 设计了一套极其巧妙的**红绿灯规则（头部标记）**：

#### 规则表（请配合 CS50 的灯泡概念看）：

|                  |                  |                                        |                                         |
| ---------------- | ---------------- | -------------------------------------- | --------------------------------------- |
| 字符需要的字节数         | 第一字节的样子 (Header) | 后面字节的样子 (Data)                         | 解释                                      |
| **1 字节** (ASCII) | **0**xxxxxxx     | (无)                                    | 只要开头是 **0**，它就是 ASCII，只读这 1 个字节。        |
| **2 字节**         | **110**xxxxx     | **10**xxxxxx                           | 开头是 **110**，说明连同它自己在内，后面一共要读 **2** 个字节。 |
| **3 字节** (汉字)    | **1110**xxxx     | **10**xxxxxx **10**xxxxxx              | 开头是 **1110**，说明一共要读 **3** 个字节。          |
| **4 字节** (Emoji) | **11110**xxx     | **10**xxxxxx **10**xxxxxx **10**xxxxxx | 开头是 **11110**，说明一共要读 **4** 个字节。         |

1. **完全兼容 ASCII**：看第一行，如果第一位是 0，它就和 ASCII 码一模一样！这就是为什么老美国的文件在 UTF-8 系统里能直接打开，乱码都不会有。
    
2. **后续字节标记**：多字节字符中，除了领头的那个字节，后面跟着的“跟班字节”全部都以 10 开头。
    
    - 这样做的好处是：如果数据传输中丢了一个字节，计算机很容易发现（因为它看到了不该出现的 10 开头或者缺少了 10 开头），不会导致后面所有的文字全部错位。

#### 例子 1：字母 'A'

- Unicode 编号：65
    
- 二进制：1000001 (7位)
    
- **UTF-8 编码**：
    
    - 因为它小，属于 ASCII 范围，套用 1 字节规则：0xxxxxxx
        
    - 结果：01000001 (和 ASCII 一模一样)
        

#### 例子 2：汉字 '中'

- Unicode 编号：20013 (十六进制 4E2D)
    
- 二进制：0100 1110 0010 1101 (15位)
    
- **UTF-8 编码过程**：
    
    1. 15 位二进制，这就需要 **3 个字节** 的容器来装（因为 2 字节存不下，3 字节能存 16 位以上）。
        
    2. 拿出 3 字节的模板：  
        1110xxxx 10xxxxxx 10xxxxxx
        
    3. 把 '中' 的二进制（0100111000101101）从后往前填入那些 x 的位置。
        
    4. **填完后**：  
        11100100 10111000 10101101
        
    5. 这就是 '中' 字在硬盘里的样子。

## 算法
这也是CS50极其标志性的环节了，不出意外，戴维教授今年也手撕了一次书来极其生动的表述。

![](/images/Pasted image 20251117201133.png)

![](/images/Pasted image 20251117201506.png)

这一段可以说是整个CS50给我们带来的震撼所在，知识的具象化在此体现的淋漓尽致。

如果普通的课堂，可能老师会讲解：
- "二分查找的时间复杂度是 O(log n)"

是的，抽象化的知识是更好记忆和描述的，但是知识何尝不也是可以冲击我们的视听感官的呢？

手撕电话簿，从 1000 页 → 500 页 → 250 页 → 125 页...
我们可以很直观的感受到，我们平时每一个不经意的细节，都有算法的影子。

极其推荐大家可以反复观看这一段关于算法的描述。对我们理解算法的帮助确实难以用语言描述。

Tips：十年后大家可能会忘记二分算法，但是永远忘不了那本被撕碎的电话簿（笑）



---
**伪代码：程序设计的好习惯：先构想设计，而不是上来就写**


这个部分大家也可以详细的反复体会









_"Computers speak in numbers"_ — David Malan